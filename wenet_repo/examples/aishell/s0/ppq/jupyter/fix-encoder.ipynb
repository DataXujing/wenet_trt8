{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f56c63-ad33-45bd-8d77-2b6873aecf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "import numpy as np\n",
    "import onnxsim\n",
    "from onnx import shape_inference,numpy_helper\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bca3c22-21a2-4d98-b94c-c2ebf22120ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "int64attr = OrderedDict([('to', 7)])\n",
    "int32attr = OrderedDict([('to', 6)])\n",
    "boolattr = OrderedDict([('to', 9)])\n",
    "float32attr = OrderedDict([('to', 1)])\n",
    "shape_dict = {\"speech\": [4, 64, 80],\"speech_lengths\": [4,]}\n",
    "\n",
    "shape_infer = True\n",
    "check = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7d2660-7fca-4e06-86fd-2dde50d0d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gs(gs_graph):\n",
    "    out_onnx = gs.export_onnx(gs_graph)\n",
    "    out_onnx = shape_inference.infer_shapes(out_onnx)\n",
    "    onnx.checker.check_model(out_onnx)\n",
    "    return out_onnx\n",
    "\n",
    "def findSlice(node):\n",
    "    Slice = None\n",
    "    try:\n",
    "        for i in range(20):\n",
    "            nextop = node.o(i)\n",
    "    except Exception as e:\n",
    "        return Slice\n",
    "    else:\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8ab816-7cca-4490-8b6c-325adb4babef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cd ../ && trtexec --workspace=8192 \\\n",
    "#  --onnx=./orin_onnx/encoder.onnx \\\n",
    "#  --optShapes=speech:4x64x80,speech_lengths:4 \\\n",
    "#  --maxShapes=speech:16x256x80,speech_lengths:16 \\\n",
    "#  --minShapes=speech:1x16x80,speech_lengths:1 \\\n",
    "#  --shapes=speech:1x16x80,speech_lengths:1 \\\n",
    "#  --saveEngine=./engine/encoder.plan \\\n",
    "#  2>&1 | tee ./log/encoder.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f8932b-e1c9-4147-959e-f8ba734348a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ox = '../orin_onnx/encoder.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5507ae5-567d-40e4-954e-de524d0a372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_graph = onnx.load(ox)\n",
    "gs_graph = gs.import_onnx(onnx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b91a1e3-d399-48fe-9e3a-96508064d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_slice(gs_graph):\n",
    "    Unsqueeze = Not = Slice = None\n",
    "    for node in gs_graph.nodes:\n",
    "        if node.op == 'Unsqueeze' and node.o().op == 'Not' and node.o().o().op == 'Slice':\n",
    "            Unsqueeze = node\n",
    "            Not = node.o()\n",
    "            Slice = node.o().o()\n",
    "    return Unsqueeze,Not,Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c4e80f-2c5d-4aa9-9cee-f1bc6f5eea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unsqueeze,Not,Slice = deal_slice(gs_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b00cd6-30b7-424c-b41a-8472e256130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cast0_input = Not.outputs[0]\n",
    "Cast0_output =  gs.Variable(name=\"Cast0_output\", dtype=np.dtype(np.int32), shape=None)\n",
    "\n",
    "Cast0 = gs.Node(name='Add_Cast0',op='Cast',\n",
    "               inputs=[Cast0_input],\n",
    "               outputs=[Cast0_output],\n",
    "               attrs=int32attr)\n",
    "gs_graph.nodes.append(Cast0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560ab0e7-4208-4c0f-96ac-419dcf8649f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Slice.inputs[0] = Cast0_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e44ee9-920d-4b56-9b89-8ccca1405009",
   "metadata": {},
   "outputs": [],
   "source": [
    "Slice0_output = Slice.outputs[0].copy()\n",
    "Slice0_output.name = 'Slice0_output'\n",
    "\n",
    "Cast1_input = Slice0_output\n",
    "Cast1_output = Slice.outputs[0]\n",
    "\n",
    "Slice.outputs[0] = Slice0_output\n",
    "\n",
    "Cast1 = gs.Node(name='Add_Cast1',op='Cast',\n",
    "               inputs=[Cast1_input],\n",
    "               outputs=[Cast1_output],\n",
    "               attrs=boolattr)\n",
    "gs_graph.nodes.append(Cast1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50dca3cc-6c13-48f5-aed9-ac0f03d86004",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_graph.cleanup().toposort()\n",
    "onnx.save(gs.export_onnx(gs_graph),'../tmp_onnx/encoder_fix_slice.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39211d53-4d6a-4f54-a9ed-4d5a51a0fc60",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8401] # trtexec --workspace=8192 --onnx=./tmp_onnx/encoder.onnx --optShapes=speech:4x64x80,speech_lengths:4 --maxShapes=speech:16x256x80,speech_lengths:16 --minShapes=speech:1x16x80,speech_lengths:1 --shapes=speech:1x16x80,speech_lengths:1 --saveEngine=./engine/encoder.plan\n",
      "[06/18/2022-09:54:45] [W] --workspace flag has been deprecated by --memPoolSize flag.\n",
      "[06/18/2022-09:54:45] [I] === Model Options ===\n",
      "[06/18/2022-09:54:45] [I] Format: ONNX\n",
      "[06/18/2022-09:54:45] [I] Model: ./tmp_onnx/encoder.onnx\n",
      "[06/18/2022-09:54:45] [I] Output:\n",
      "[06/18/2022-09:54:45] [I] === Build Options ===\n",
      "[06/18/2022-09:54:45] [I] Max batch: explicit batch\n",
      "[06/18/2022-09:54:45] [I] Memory Pools: workspace: 8192 MiB, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[06/18/2022-09:54:45] [I] minTiming: 1\n",
      "[06/18/2022-09:54:45] [I] avgTiming: 8\n",
      "[06/18/2022-09:54:45] [I] Precision: FP32\n",
      "[06/18/2022-09:54:45] [I] LayerPrecisions: \n",
      "[06/18/2022-09:54:45] [I] Calibration: \n",
      "[06/18/2022-09:54:45] [I] Refit: Disabled\n",
      "[06/18/2022-09:54:45] [I] Sparsity: Disabled\n",
      "[06/18/2022-09:54:45] [I] Safe mode: Disabled\n",
      "[06/18/2022-09:54:45] [I] DirectIO mode: Disabled\n",
      "[06/18/2022-09:54:45] [I] Restricted mode: Disabled\n",
      "[06/18/2022-09:54:45] [I] Build only: Disabled\n",
      "[06/18/2022-09:54:45] [I] Save engine: ./engine/encoder.plan\n",
      "[06/18/2022-09:54:45] [I] Load engine: \n",
      "[06/18/2022-09:54:45] [I] Profiling verbosity: 0\n",
      "[06/18/2022-09:54:45] [I] Tactic sources: Using default tactic sources\n",
      "[06/18/2022-09:54:45] [I] timingCacheMode: local\n",
      "[06/18/2022-09:54:45] [I] timingCacheFile: \n",
      "[06/18/2022-09:54:45] [I] Input(s)s format: fp32:CHW\n",
      "[06/18/2022-09:54:45] [I] Output(s)s format: fp32:CHW\n",
      "[06/18/2022-09:54:45] [I] Input build shape: speech_lengths=1+4+16\n",
      "[06/18/2022-09:54:45] [I] Input build shape: speech=1x16x80+4x64x80+16x256x80\n",
      "[06/18/2022-09:54:45] [I] Input calibration shapes: model\n",
      "[06/18/2022-09:54:45] [I] === System Options ===\n",
      "[06/18/2022-09:54:45] [I] Device: 0\n",
      "[06/18/2022-09:54:45] [I] DLACore: \n",
      "[06/18/2022-09:54:45] [I] Plugins:\n",
      "[06/18/2022-09:54:45] [I] === Inference Options ===\n",
      "[06/18/2022-09:54:45] [I] Batch: Explicit\n",
      "[06/18/2022-09:54:45] [I] Input inference shape: speech_lengths=1\n",
      "[06/18/2022-09:54:45] [I] Input inference shape: speech=1x16x80\n",
      "[06/18/2022-09:54:45] [I] Iterations: 10\n",
      "[06/18/2022-09:54:45] [I] Duration: 3s (+ 200ms warm up)\n",
      "[06/18/2022-09:54:45] [I] Sleep time: 0ms\n",
      "[06/18/2022-09:54:45] [I] Idle time: 0ms\n",
      "[06/18/2022-09:54:45] [I] Streams: 1\n",
      "[06/18/2022-09:54:45] [I] ExposeDMA: Disabled\n",
      "[06/18/2022-09:54:45] [I] Data transfers: Enabled\n",
      "[06/18/2022-09:54:45] [I] Spin-wait: Disabled\n",
      "[06/18/2022-09:54:45] [I] Multithreading: Disabled\n",
      "[06/18/2022-09:54:45] [I] CUDA Graph: Disabled\n",
      "[06/18/2022-09:54:45] [I] Separate profiling: Disabled\n",
      "[06/18/2022-09:54:45] [I] Time Deserialize: Disabled\n",
      "[06/18/2022-09:54:45] [I] Time Refit: Disabled\n",
      "[06/18/2022-09:54:45] [I] Inputs:\n",
      "[06/18/2022-09:54:45] [I] === Reporting Options ===\n",
      "[06/18/2022-09:54:45] [I] Verbose: Disabled\n",
      "[06/18/2022-09:54:45] [I] Averages: 10 inferences\n",
      "[06/18/2022-09:54:45] [I] Percentile: 99\n",
      "[06/18/2022-09:54:45] [I] Dump refittable layers:Disabled\n",
      "[06/18/2022-09:54:45] [I] Dump output: Disabled\n",
      "[06/18/2022-09:54:45] [I] Profile: Disabled\n",
      "[06/18/2022-09:54:45] [I] Export timing to JSON file: \n",
      "[06/18/2022-09:54:45] [I] Export output to JSON file: \n",
      "[06/18/2022-09:54:45] [I] Export profile to JSON file: \n",
      "[06/18/2022-09:54:45] [I] \n",
      "[06/18/2022-09:54:45] [I] === Device Information ===\n",
      "[06/18/2022-09:54:45] [I] Selected Device: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "[06/18/2022-09:54:45] [I] Compute Capability: 8.6\n",
      "[06/18/2022-09:54:45] [I] SMs: 40\n",
      "[06/18/2022-09:54:45] [I] Compute Clock Rate: 1.56 GHz\n",
      "[06/18/2022-09:54:45] [I] Device Global Memory: 7982 MiB\n",
      "[06/18/2022-09:54:45] [I] Shared Memory per SM: 100 KiB\n",
      "[06/18/2022-09:54:45] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[06/18/2022-09:54:45] [I] Memory Clock Rate: 7.001 GHz\n",
      "[06/18/2022-09:54:45] [I] \n",
      "[06/18/2022-09:54:45] [I] TensorRT version: 8.4.1\n",
      "[06/18/2022-09:54:45] [I] [TRT] [MemUsageChange] Init CUDA: CPU +328, GPU +0, now: CPU 336, GPU 300 (MiB)\n",
      "[06/18/2022-09:54:46] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +327, GPU +104, now: CPU 682, GPU 404 (MiB)\n",
      "[06/18/2022-09:54:46] [I] Start parsing network model\n",
      "[06/18/2022-09:54:46] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/18/2022-09:54:46] [I] [TRT] Input filename:   ./tmp_onnx/encoder.onnx\n",
      "[06/18/2022-09:54:46] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[06/18/2022-09:54:46] [I] [TRT] Opset version:    13\n",
      "[06/18/2022-09:54:46] [I] [TRT] Producer name:    pytorch\n",
      "[06/18/2022-09:54:46] [I] [TRT] Producer version: 1.11.0\n",
      "[06/18/2022-09:54:46] [I] [TRT] Domain:           \n",
      "[06/18/2022-09:54:46] [I] [TRT] Model version:    0\n",
      "[06/18/2022-09:54:46] [I] [TRT] Doc string:       \n",
      "[06/18/2022-09:54:46] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/18/2022-09:54:46] [W] [TRT] onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[06/18/2022-09:54:52] [I] Finish parsing network model\n",
      "[06/18/2022-09:54:58] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +871, GPU +378, now: CPU 1703, GPU 782 (MiB)\n",
      "[06/18/2022-09:54:58] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +128, GPU +60, now: CPU 1831, GPU 842 (MiB)\n",
      "[06/18/2022-09:54:58] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/18/2022-09:54:59] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:54:59] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:54:59] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:55:14] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:55:14] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:55:14] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:56:01] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:56:01] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:56:01] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:56:17] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:56:17] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:56:17] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:56:33] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:56:33] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:56:33] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:56:49] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:56:49] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:56:49] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:57:05] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:57:05] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:57:05] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:57:21] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:57:21] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:57:21] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:57:37] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:57:37] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:57:37] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:57:53] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:57:53] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:57:53] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:58:09] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:58:09] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:58:09] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:58:25] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:58:25] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:58:25] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:58:40] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:58:40] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:58:40] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:58:56] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:58:56] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:58:56] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [I] [TRT] Detected 2 inputs and 5 output network tensors.\n",
      "[06/18/2022-09:59:11] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:11] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 1 (SHAPE speech))\n",
      "[06/18/2022-09:59:12] [W] [TRT]  (# 0 (SHAPE speech))\n",
      "[06/18/2022-09:59:16] [I] [TRT] Total Host Persistent Memory: 45968\n",
      "[06/18/2022-09:59:16] [I] [TRT] Total Device Persistent Memory: 30208\n",
      "[06/18/2022-09:59:16] [I] [TRT] Total Scratch Memory: 103121408\n",
      "[06/18/2022-09:59:16] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 4360 MiB\n",
      "[06/18/2022-09:59:16] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 106.339ms to assign 9 blocks to 230 nodes requiring 205949444 bytes.\n",
      "[06/18/2022-09:59:16] [I] [TRT] Total Activation Memory: 205949444\n",
      "[06/18/2022-09:59:16] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 3116, GPU 1518 (MiB)\n",
      "[06/18/2022-09:59:16] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +134, now: CPU 0, GPU 134 (MiB)\n",
      "[06/18/2022-09:59:16] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[06/18/2022-09:59:16] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[06/18/2022-09:59:16] [I] Engine built in 271.699 sec.\n",
      "[06/18/2022-09:59:16] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2648, GPU 1266 (MiB)\n",
      "[06/18/2022-09:59:16] [I] [TRT] Loaded engine size: 141 MiB\n",
      "[06/18/2022-09:59:16] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2782, GPU 1410 (MiB)\n",
      "[06/18/2022-09:59:16] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +133, now: CPU 0, GPU 133 (MiB)\n",
      "[06/18/2022-09:59:16] [I] Engine deserialized in 0.0758521 sec.\n",
      "[06/18/2022-09:59:16] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2782, GPU 1410 (MiB)\n",
      "[06/18/2022-09:59:19] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +196, now: CPU 0, GPU 329 (MiB)\n",
      "[06/18/2022-09:59:19] [I] Using random values for input speech\n",
      "[06/18/2022-09:59:19] [I] Created input binding for speech with dimensions 1x16x80\n",
      "[06/18/2022-09:59:19] [I] Using random values for input speech_lengths\n",
      "[06/18/2022-09:59:19] [I] Created input binding for speech_lengths with dimensions 1\n",
      "[06/18/2022-09:59:19] [I] Using random values for output encoder_out_lens\n",
      "[06/18/2022-09:59:19] [I] Created output binding for encoder_out_lens with dimensions 1\n",
      "[06/18/2022-09:59:19] [I] Using random values for output encoder_out\n",
      "[06/18/2022-09:59:19] [I] Created output binding for encoder_out with dimensions 1x3x256\n",
      "[06/18/2022-09:59:19] [I] Using random values for output ctc_log_probs\n",
      "[06/18/2022-09:59:19] [I] Created output binding for ctc_log_probs with dimensions 1x3x4233\n",
      "[06/18/2022-09:59:19] [I] Using random values for output beam_log_probs\n",
      "[06/18/2022-09:59:19] [I] Created output binding for beam_log_probs with dimensions 1x3x10\n",
      "[06/18/2022-09:59:19] [I] Using random values for output beam_log_probs_idx\n",
      "[06/18/2022-09:59:19] [I] Created output binding for beam_log_probs_idx with dimensions 1x3x10\n",
      "[06/18/2022-09:59:19] [I] Starting inference\n",
      "[06/18/2022-09:59:22] [I] Warmup completed 50 queries over 200 ms\n",
      "[06/18/2022-09:59:22] [I] Timing trace has 724 queries over 3.00668 s\n",
      "[06/18/2022-09:59:22] [I] \n",
      "[06/18/2022-09:59:22] [I] === Trace details ===\n",
      "[06/18/2022-09:59:22] [I] Trace averages of 10 runs:\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.96483 ms - Host latency: 3.98596 ms (enqueue 3.91731 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.9519 ms - Host latency: 3.97197 ms (enqueue 3.90132 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.93401 ms - Host latency: 3.95707 ms (enqueue 3.88394 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.90509 ms - Host latency: 3.92708 ms (enqueue 3.85509 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.96144 ms - Host latency: 3.98492 ms (enqueue 3.91204 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.95648 ms - Host latency: 3.97773 ms (enqueue 3.90669 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.92262 ms - Host latency: 3.94457 ms (enqueue 3.8685 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.9595 ms - Host latency: 3.98159 ms (enqueue 3.90791 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.93761 ms - Host latency: 3.9581 ms (enqueue 3.88378 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.92991 ms - Host latency: 3.95204 ms (enqueue 3.88064 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.85248 ms - Host latency: 3.87415 ms (enqueue 3.80428 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.9343 ms - Host latency: 3.95806 ms (enqueue 3.88394 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.95254 ms - Host latency: 3.97335 ms (enqueue 3.90117 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 4.13173 ms - Host latency: 4.15565 ms (enqueue 4.08943 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 5.22563 ms - Host latency: 5.24971 ms (enqueue 5.19295 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 4.57438 ms - Host latency: 4.5986 ms (enqueue 4.53973 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 5.33688 ms - Host latency: 5.3658 ms (enqueue 5.31106 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.9441 ms - Host latency: 3.96388 ms (enqueue 3.89579 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.92075 ms - Host latency: 3.94237 ms (enqueue 3.87034 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 5.45693 ms - Host latency: 5.4912 ms (enqueue 5.42946 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 4.4043 ms - Host latency: 4.4302 ms (enqueue 4.35985 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.76416 ms - Host latency: 3.78452 ms (enqueue 3.71564 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.7736 ms - Host latency: 3.79454 ms (enqueue 3.72581 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.96383 ms - Host latency: 3.98497 ms (enqueue 3.91726 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.78666 ms - Host latency: 3.80726 ms (enqueue 3.73682 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.85955 ms - Host latency: 3.88075 ms (enqueue 3.8113 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.88163 ms - Host latency: 3.90096 ms (enqueue 3.83243 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.9387 ms - Host latency: 3.96044 ms (enqueue 3.89293 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.98434 ms - Host latency: 4.00509 ms (enqueue 3.93876 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 4.04376 ms - Host latency: 4.06403 ms (enqueue 3.9967 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 4.10886 ms - Host latency: 4.13242 ms (enqueue 4.06279 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.82072 ms - Host latency: 3.8412 ms (enqueue 3.76958 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.85281 ms - Host latency: 3.87385 ms (enqueue 3.80562 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.99546 ms - Host latency: 4.01792 ms (enqueue 3.94595 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 5.64476 ms - Host latency: 5.67991 ms (enqueue 5.62106 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 5.94979 ms - Host latency: 5.98378 ms (enqueue 5.92612 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 5.97827 ms - Host latency: 6.00913 ms (enqueue 5.959 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 6.54285 ms - Host latency: 6.5772 ms (enqueue 6.5266 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 6.77037 ms - Host latency: 6.81084 ms (enqueue 6.75363 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.96536 ms - Host latency: 3.98507 ms (enqueue 3.91709 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.77112 ms - Host latency: 3.79225 ms (enqueue 3.72217 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.82505 ms - Host latency: 3.84496 ms (enqueue 3.7776 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.90339 ms - Host latency: 3.92493 ms (enqueue 3.8517 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.89617 ms - Host latency: 3.91545 ms (enqueue 3.84585 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.81082 ms - Host latency: 3.83259 ms (enqueue 3.76514 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.9352 ms - Host latency: 3.95618 ms (enqueue 3.88474 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.90669 ms - Host latency: 3.92583 ms (enqueue 3.8564 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.89995 ms - Host latency: 3.92131 ms (enqueue 3.8501 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.97813 ms - Host latency: 3.99871 ms (enqueue 3.92712 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.89558 ms - Host latency: 3.91582 ms (enqueue 3.84683 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.91211 ms - Host latency: 3.93228 ms (enqueue 3.85955 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.94507 ms - Host latency: 3.9644 ms (enqueue 3.89409 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.88562 ms - Host latency: 3.90632 ms (enqueue 3.8333 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.87671 ms - Host latency: 3.89819 ms (enqueue 3.82961 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.87952 ms - Host latency: 3.90129 ms (enqueue 3.82842 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.88972 ms - Host latency: 3.90989 ms (enqueue 3.83855 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.91855 ms - Host latency: 3.93792 ms (enqueue 3.86819 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.90801 ms - Host latency: 3.92756 ms (enqueue 3.85938 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.88381 ms - Host latency: 3.90374 ms (enqueue 3.83188 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.9738 ms - Host latency: 3.99482 ms (enqueue 3.92654 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.84927 ms - Host latency: 3.86975 ms (enqueue 3.80112 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.93804 ms - Host latency: 3.95696 ms (enqueue 3.88826 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.87419 ms - Host latency: 3.89763 ms (enqueue 3.8272 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.87195 ms - Host latency: 3.89067 ms (enqueue 3.82283 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.94016 ms - Host latency: 3.95898 ms (enqueue 3.88953 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.8646 ms - Host latency: 3.88389 ms (enqueue 3.81538 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.89199 ms - Host latency: 3.91494 ms (enqueue 3.84148 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.84373 ms - Host latency: 3.86274 ms (enqueue 3.79587 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.83279 ms - Host latency: 3.85181 ms (enqueue 3.7834 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.84473 ms - Host latency: 3.86521 ms (enqueue 3.79722 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.82795 ms - Host latency: 3.84692 ms (enqueue 3.77942 ms)\n",
      "[06/18/2022-09:59:22] [I] Average on 10 runs - GPU latency: 3.89644 ms - Host latency: 3.91624 ms (enqueue 3.85181 ms)\n",
      "[06/18/2022-09:59:22] [I] \n",
      "[06/18/2022-09:59:22] [I] === Performance summary ===\n",
      "[06/18/2022-09:59:22] [I] Throughput: 240.797 qps\n",
      "[06/18/2022-09:59:22] [I] Latency: min = 3.50854 ms, max = 7.23718 ms, mean = 4.1628 ms, median = 3.94836 ms, percentile(99%) = 6.7467 ms\n",
      "[06/18/2022-09:59:22] [I] Enqueue Time: min = 3.45312 ms, max = 7.18103 ms, mean = 4.09445 ms, median = 3.87648 ms, percentile(99%) = 6.69495 ms\n",
      "[06/18/2022-09:59:22] [I] H2D Latency: min = 0.00671387 ms, max = 0.0267334 ms, mean = 0.00981262 ms, median = 0.0090332 ms, percentile(99%) = 0.0212402 ms\n",
      "[06/18/2022-09:59:22] [I] GPU Compute Time: min = 3.49194 ms, max = 7.1886 ms, mean = 4.14049 ms, median = 3.92651 ms, percentile(99%) = 6.71924 ms\n",
      "[06/18/2022-09:59:22] [I] D2H Latency: min = 0.00878906 ms, max = 0.0429688 ms, mean = 0.0124976 ms, median = 0.0107422 ms, percentile(99%) = 0.0352783 ms\n",
      "[06/18/2022-09:59:22] [I] Total Host Walltime: 3.00668 s\n",
      "[06/18/2022-09:59:22] [I] Total GPU Compute Time: 2.99771 s\n",
      "[06/18/2022-09:59:22] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.\n",
      "[06/18/2022-09:59:22] [W]   If not already in use, --useCudaGraph (utilize CUDA graphs where possible) may increase the throughput.\n",
      "[06/18/2022-09:59:22] [W] * GPU compute time is unstable, with coefficient of variance = 17.17%.\n",
      "[06/18/2022-09:59:22] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[06/18/2022-09:59:22] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[06/18/2022-09:59:22] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8401] # trtexec --workspace=8192 --onnx=./tmp_onnx/encoder.onnx --optShapes=speech:4x64x80,speech_lengths:4 --maxShapes=speech:16x256x80,speech_lengths:16 --minShapes=speech:1x16x80,speech_lengths:1 --shapes=speech:1x16x80,speech_lengths:1 --saveEngine=./engine/encoder.plan\n"
     ]
    }
   ],
   "source": [
    "# !cd ../ && trtexec --workspace=8192 \\\n",
    "#  --onnx=./tmp_onnx/encoder.onnx \\\n",
    "#  --optShapes=speech:4x64x80,speech_lengths:4 \\\n",
    "#  --maxShapes=speech:16x256x80,speech_lengths:16 \\\n",
    "#  --minShapes=speech:1x16x80,speech_lengths:1 \\\n",
    "#  --shapes=speech:1x16x80,speech_lengths:1 \\\n",
    "#  --saveEngine=./engine/encoder.plan \\\n",
    "#  2>&1 | tee ./log/encoder.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5aca4bc-8600-4f56-8eb4-0752da2842a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Slice_x = None\n",
    "for node in gs_graph.nodes:\n",
    "    if node.op == 'Slice' and findSlice(node):\n",
    "        Slice_x = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc872ff0-c678-4b0d-b5ce-c1818a15dfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slice_59 (Slice)\n",
       "\tInputs: [\n",
       "\t\tVariable (onnx::Slice_554): (shape=None, dtype=None)\n",
       "\t\tConstant (onnx::Slice_3311): (shape=[1], dtype=<class 'numpy.int64'>)\n",
       "\t\tVariable (onnx::Slice_560): (shape=None, dtype=None)\n",
       "\t\tConstant (onnx::Slice_3312): (shape=[1], dtype=<class 'numpy.int64'>)\n",
       "\t\tVariable (onnx::Slice_563): (shape=None, dtype=None)\n",
       "\t]\n",
       "\tOutputs: [\n",
       "\t\tVariable (input.19): (shape=None, dtype=None)\n",
       "\t]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Slice_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b94264e-d04c-4a01-87f6-6d022a234e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5000x256 = np.load('5000x256')[0]\n",
    "table5000x256.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089b266f-fc5d-4735-a49f-49146432efec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84147096, 0.54030234, 0.8019618 , 0.59737533, 0.76172036,\n",
       "       0.6479059 , 0.7214141 , 0.6925039 , 0.68156135, 0.731761  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5000x256[1,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75a689db-0aea-46a2-b6f3-430ff08d0c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable (onnx::Slice_560): (shape=None, dtype=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliceTensor = Slice_x.inputs[2]\n",
    "sliceTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6ea65a5-5c19-4fd4-8d94-7acd23202264",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = gs.Constant(name='Constant-0',values=np.array([0]))\n",
    "one = gs.Constant(name='Constant-1',values=np.array([1]))\n",
    "three = gs.Constant(name='Constant-3',values=np.array([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13125e84-55bd-4501-baa6-91a20b630428",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in range(1,24,2):\n",
    "    trashNode = Slice_x.o(i).o().o()\n",
    "    factor256x256 = Slice_x.o(i).inputs[1].values\n",
    "    newTable = table5000x256@factor256x256\n",
    "    newTable = newTable.transpose().reshape(1,4,64,5000)\n",
    "    constantData = gs.Constant(f'Data-{j}',np.ascontiguousarray(newTable))\n",
    "    sliceV = gs.Variable(f'sliceData-{j}',np.dtype(np.float32),[1,4,64,'slice'])\n",
    "    sliceN = gs.Node('Slice',f'SliceN-{j}',\n",
    "                    inputs = [constantData,zero,sliceTensor,three,one],\n",
    "                    outputs=[sliceV])\n",
    "    j+=1\n",
    "    gs_graph.nodes.append(sliceN)\n",
    "    Slice_x.o(i).o().o().o().inputs[1] = sliceV\n",
    "    trashNode.outputs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48755be7-d4df-4df2-aed1-80b397d43a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_graph.cleanup().toposort()\n",
    "onnx.save(gs.export_onnx(gs_graph),'../tmp_onnx/encoder_fix_5000x256.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bc16e-bbcf-4b78-93b2-fbb865ed9c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
