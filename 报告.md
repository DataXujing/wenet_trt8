## 总述
本项目利用 TensorRT 加速 ASR 模型 WeNet，对拆分后 Encoder 和 Decoder 分别进行量化 FP16 推理，INT8 对称量化，对部分算子实现了高效的 plugin，在 batch 1-8 下精度对比原始模型有微弱降低但是推理速度大大提升。











- 1. 
- TODO：优化效果（精度和加速比），简单给出关键的数字即可，在这里不必详细展开
- TODO：在Docker里面代码编译、运行步骤的完整说明，请做到只要逐行运行你给的命令，就能把代码跑起来，比如从docker pull开始

## 原始模型
### 模型简介

WeNet 是一款面向工业落地应用的语音识别工具包，提供了从语音识别模型的训练到部署的一条龙服务，其主要特点如下：

* 使用 conformer 网络结构和 CTC/attention loss 联合优化方法，统一的流式/非流式语音识别方案，具有业界一流的识别效果。
* 提供云上和端上直接部署的方案，最小化模型训练和产品落地之间的工程工作。
* 框架简洁，模型训练部分完全基于 pytorch 生态，不依赖于 kaldi 等复杂的工具。
* 详细的注释和文档，非常适合用于学习端到端语音识别的基础知识和实现细节。
* 支持时间戳，对齐，端点检测，语言模型等相关功能。
* github 共计 2k 星标，广泛的应用于喜马拉雅、作业帮、京东、腾讯等数百家公司，业务涵盖了智能车载、智能家居、智能客服、音频内容生产、直播、会议等语音识别应用场景。

本项目的模型使用预训练模型导出onnx，然后进行TRT部署。预训练模型链接 http://mobvoi-speech-public.ufile.ucloud.cn/public/wenet/aishell/20210204_conformer_exp.tar.gz，预训练模型方法导出参考[WeNet手册](https://wenet.org.cn/wenet/tutorial_aishell.html)。

训练等相关信息请参考官方：https://github.com/wenet-e2e/wenet。



### 模型优化的难点

WeNet 模型分为 encoder 和 decoder 两个部分。其中，encoder 主要使用了 conv 和 self-attention 结构，而 decoder 使用了 self-attention 和 cross-attention 结构。在模型转换和使用过程中存在以下问题：
* 由于是 pytorch 导出 onnx 模型，因此 onnx 模型中使用了大量小算子拼凑出 attention 功能。
* 在使用 trtexec 直接解析 decoder 模型时，在RTX 3080Ti 12G显卡上会出现显存不足的错误。
* 使用 Half 数据类型进行推理，encoder 和 decoder 的精度损失严重。

针对以上问题，本项目采用以下方法进行模型优化。
* 合并 onnx 模型中的小算子，使用MultiHeadAttn、LayerNorm等大算子替代原始小算子。
* 使用 trtexec 解析替换大算子的模型。
* 分析各个节点输出，定位误差大的节点，并使用高精度进行计算。
* 尝试模型量化，使用 INT8 进行推理，保证精度的情况下，进一步加快速度。

模型可以通过官方提供的代码导出到 ONNX，但 ONNX 转 TensorRT 时，存在掉精度，性能不够好的问题。具体有：
- FP16 时模型精度掉的严重，模型中 Reduce 相关的算子可能存在数据溢出的问题
- LayerNorm 算子被打散成 10 多个小算子，而 TensorRT 对这些小算子的 fusion 不够好，导致掉精度以及速度不够快
- 导出的 ONNX 模型中存在重复计算的部分，比如生成 mask, pos_emb, 导致多余的重复计算
- TensorRT对部分算子的融合不够好，比如 bias_residual_layernorm, masked_softmax, multi-head attention 
- TessorRT对部分算子的实现不够快，比如 depthwise_convolution, logsoftmax
- INT8 PTQ掉点
- 存在大量细碎的kernel，导致 kernel lauch 耗时占比较高
- ......

这些问题导致该模型要达到极致的性能需要做很多深度的工程优化，存在较大的开发工作量和较大的难度，有一定的挑战性。

## 优化过程（TODO）
优化技术：

1. 针对 pytorch 导出的 encoder Slice 算子不支持 bool 输出进行了输入输出的强制转换。(与初赛一致)
2. 针对 encoder 中由 torch.view 算子导出的多个琐碎算子进行了整合，使用 Flatten Transpose 算子进行替换，大大减少了琐碎算子计算。
   ![image-20220626185714687](/home/ubuntu/.config/Typora/typora-user-images/image-20220626185714687.png)
3. 针对 encoder 中两个 Slice 算子连接使用单 Slice 替换。
   ![image-20220626185753417](/home/ubuntu/.config/Typora/typora-user-images/image-20220626185753417.png)
4. 针对大矩阵和固定矩阵乘法计算连接 Slice 的情况对固定的运算进行提前计算，减少了运行时多与的计算。
   ![image-20220626185835504](/home/ubuntu/.config/Typora/typora-user-images/image-20220626185835504.png)
5. 对 LayerNorm 操作的大量算子使用 int8/fp16/fp32 高效 Plugin 替换。
   ![image-20220626190114375](/home/ubuntu/.config/Typora/typora-user-images/image-20220626190114375.png)
6. 针对 Attention Mask  Softmax 部分使用 AttentionMaskSoftmaxPlugin 进行替换。
   ![image-20220626190530810](/home/ubuntu/.config/Typora/typora-user-images/image-20220626190530810.png)
7. 对于所有的 mask 计算加入到输入，提前计算好根据输入的 mask，减少在运行时额外计算。
8. 根据 FastTransformer 实现上述 Plugin，实现 int8/fp16/fp32 的模板。使用 onnxruntime 对所有 Conv/MatMul节点 weights 进行 int8 量化，对 Softmax/bias 不进行量化，对 Plugin 包含的节点进行量化。同时使用 ppq 中的 TensorRT quant 配置对 encoder decoder 全部节点进行自适应量化，对 Plugin 包含的节点选择 fp16/fp32 构建。



这一部分是报告的主体。请把自己假定为老师，为TensorRT的初学者讲述如何从原始模型出发，经过一系列开发步骤，得到优化后的TensorRT模型。  

建议：
- 分步骤讲清楚开发过程
- 最好能介绍为什么需要某个特别步骤，通过这个特别步骤解决了什么问题
  - 比如，通过Nsight Systems绘制timeline做了性能分析，发现attention时间占比高且有优化空间（贴图展示分析过程），所以决定要写plugin。然后介绍plugin的设计与实现，并在timeline上显示attention这一部分的性能改进。

## 精度与加速效果（TODO)
Batch-1

===========================================================================

Overall -> 6.06 % N=104765 C=98566 S=6057 D=142 I=152
Mandarin -> 6.06 % N=104762 C=98566 S=6054 D=142 I=152
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine1
Method: attention_rescoring
Mean(ms): 
encoder ——— 7.8731064322742474
decoder ——— 3.5666254789576364

===========================================================================

Overall -> 4.63 % N=104765 C=100011 S=4639 D=115 I=98
Mandarin -> 4.63 % N=104762 C=100011 S=4636 D=115 I=98
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine2
Method: attention_rescoring
Mean(ms): 
encoder ——— 18.403791837792642
decoder ——— 20.84104462876254

===========================================================================

Batch-4

===========================================================================

Overall -> 6.25 % N=104765 C=98362 S=6252 D=151 I=150
Mandarin -> 6.25 % N=104762 C=98362 S=6249 D=151 I=150
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine1
Method: attention_rescoring
Mean(ms): 
encoder ——— 11.380562555183946
decoder ——— 9.271438920847269

===========================================================================

Overall -> 4.78 % N=104765 C=99856 S=4790 D=119 I=104
Mandarin -> 4.78 % N=104762 C=99856 S=4787 D=119 I=104
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine2
Method: attention_rescoring
Mean(ms): 
encoder ——— 23.53952972742475
decoder ——— 22.358369596432553

Batch-8

===========================================================================

Overall -> 6.43 % N=104765 C=98190 S=6418 D=157 I=162
Mandarin -> 6.43 % N=104762 C=98190 S=6415 D=157 I=162
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine1
Method: attention_rescoring
Mean(ms): 
encoder ——— 15.680679668896321
decoder ——— 17.830058614269788

===========================================================================

Overall -> 4.82 % N=104765 C=99825 S=4819 D=121 I=113
Mandarin -> 4.82 % N=104762 C=99825 S=4816 D=121 I=113
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine2
Method: attention_rescoring
Mean(ms): 
encoder ——— 26.169091756967667
decoder ——— 22.91126618394649

Batch-16

===========================================================================

Overall -> 6.73 % N=104765 C=97876 S=6713 D=176 I=157
Mandarin -> 6.72 % N=104762 C=97876 S=6710 D=176 I=157
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine1
Method: attention_rescoring
Mean(ms): 
encoder ——— 26.150049973273937
decoder ——— 36.37694616035635

===========================================================================

Overall -> 4.85 % N=104765 C=99799 S=4845 D=121 I=116
Mandarin -> 4.85 % N=104762 C=99799 S=4842 D=121 I=116
Other -> 100.00 % N=3 C=0 S=3 D=0 I=0

===========================================================================
Backend: engine2
Method: attention_rescoring
Mean(ms): 
encoder ——— 30.779932178173716
decoder ——— 23.61681683741648



| model            | batch 1            |      | batch 4            | batch 8            | batch 16           | error b1 | error b4 | error b8 | error b16 |
| ---------------- | ------------------ | ---- | ------------------ | ------------------ | ------------------ | -------- | -------- | -------- | --------- |
| original encoder | 18.403791837792642 |      | 23.53952972742475  | 26.169091756967667 | 30.779932178173716 |          |          |          |           |
| original decoder | 20.84104462876254  |      | 22.358369596432553 | 22.91126618394649  | 23.61681683741648  | 4.63     | 4.78     | 4.82     | 4.85      |
| our encoder      | 7.8731064322742474 |      | 11.380562555183946 | 15.680679668896321 | 26.150049973273937 |          |          |          |           |
| our decoder      | 3.5666254789576364 |      | 9.271438920847269  | 17.830058614269788 | 36.37694616035635  | 6.06     | 6.25     | 6.43     | 6.72      |







这一部分介绍优化模型在云主机上的运行效果，需要分两部分说明：  

- 精度：报告与原始模型进行精度对比测试的结果，验证精度达标。
  - 这里的精度测试指的是针对“原始模型”和“TensorRT优化模型”分别输出的数据（tensor）进行数值比较。请给出绝对误差和相对误差的统计结果（至少包括最大值、平均值与中位数）。
  - 使用训练好的权重和有意义的输入数据更有说服力。如果选手使用了随机权重和输入数据，请在这里注明。  
  - 在精度损失较大的情况下，鼓励选手用训练好的权重和测试数据集对模型优化前与优化后的准确度指标做全面比较，以增强说服力
- 性能：最好用图表展示不同batch size或sequence length下性能加速效果。
  - 一般用原始模型作为参考标准；若额外使用ONNX Runtime作为参考标准则更好。  
  - 一般提供模型推理时间的加速比即可；若能提供压力测试下的吞吐提升则更好。

请注意：
- 相关测试代码也需要包含在代码仓库中，可被复现。
- 请写明云主机的软件硬件环境，方便他人参考。  

## Bug报告（可选）(TODO)
提交bug是对TensorRT的另一种贡献。发现的TensorRT、或cookbook、或文档和教程相关bug，请提交到[github issues](https://github.com/NVIDIA/trt-samples-for-hackathon-cn/issues)，并请在这里给出链接。

对于每个bug，请标记上hackathon2022标签，并写好正文：
- 对于cookbook或文档和教程相关bug，说清楚问题即可，不必很详细。
- 对于TensorRT bug，首先确认在云主机上使用NGC docker + TensorRT 8.4 GA仍可复现，然后填写如下模板，并请导师复核确认（前面“评分标准”已经提到，确认有效可得附加分）：
  - Environment
    - TensorRT 8.4 GA
    - Versions of CUDA, CUBLAS, CuDNN used
    - Container used
    - NVIDIA driver version
  - Reproduction Steps
    - Provide detailed reproduction steps for the issue here, including any commands run on the command line.
  - Expected Behavior
    - Provide a brief summary of the expected behavior of the software. Provide output files or examples if possible.
  - Actual Behavior
    - Describe the actual behavior of the software and how it deviates from the expected behavior. Provide output files or examples if possible.
  - Additional Notes
    - Provide any additional context here you think might be useful for the TensorRT team to help debug this issue (such as experiments done, potential things to investigate).

## 经验与体会（可选）(TODO)
欢迎在这里总结经验，抒发感慨。